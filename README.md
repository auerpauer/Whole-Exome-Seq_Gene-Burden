This repo is no longer being updated. At some point in the future, our GH org will make code publicly available. We have no timeline on this, yet.

# Whole Exome Sequencing: Gene Burden Overview
Which genes have the most harmful variants in your set of whole exome sequencing samples?

### WHAT'S GOING ON HERE?
The general idea of "gene burden" is finding those genes that have a significant number of harmful variants among your samples. One way to do this is to compare the results from your samples against ethnic controls. This way, any genes that are highly mutated in the proper ethnic controls will not trigger a false positive in your result set.


So, what is a harmful variant? This analysis pipeline makes the following assumptions of what a harmful variant is within the constraints of VCF annotation:
1. The variant frequency in the ExAC databases is less than or equal to a given threshold. You decide what the threshold is at run-time. Suggestions: 5e-05 for dominant (homozygous) variants; 1e-04 for recessive (heterozygous) variants
2. RefSeq annotation must annotate the variant as being either exonic or splicing.
3. RefSeq annotation must NOT annotate the variant as synonymous or unknown.
4. Optionally, RefSeq annotation can be used to filter out non-frameshift deletions. You decide at run-time if you want this filtering turned on.

If all of these filters are passed, then the variant is classified according to the following criteria:
1. Loss-of-Function: RefSeq annotates the variant as "stopgain", "splicing" or "frameshift".
2. Deleterious: One of the following three conditions must be true.
*   The variant is a nonframeshift insertion or deletion.
*   The variant causes a "stoploss".
*   The variant is a non-synonymous SNV and the chosen functional deleteriousness score marks the variant as deleterious (as opposed to tolerated).
  You choose which scoring method at runtime.
3. Missense: This is the default classification, if the other two criteria are not met. Remember, we have already filtered out all variants that we are not interested in. All remaining variants must receive a classification.

The variants are further classified based on whether the variant is homozygous, heterozygous, or compound heterozygous.
Compound hets occur when the same sample contributes more than one heterozygous variant to the same gene.
Due to an assumed lack of phasing information about alleles, the Compound Het counting is somewhat simplified.

|Missense           |Deleterious           |Loss of Function|
|-------------------|----------------------|----------------|
|Total_Missense     |Total_Deleterious     |Total_LoF       |
|Het_Missense       |Het_Deleterious       |Het_LoF         |
|HomComHet_Missense |HomComHet_Deleterious |HomComHet_LoF   |
|Hom_Missense       |Hom_Deleterious       |Hom_LoF         |

Where
* Het: heterozygous mutation
* HomComHet: compound heterozygous mutations
* Hom: homozygous mutation


These variants are counted twice in two separate, but related ways:  
1. The total number of variants in a gene.  
2. The number of samples contributing one or more variants in a gene.  

Just to be clear, if a one or more samples contains more than one variant for a gene, these number are not going to be the same.

The results of these filters and classifications are saved to a tab-delimited file that contains the counts for all the above variant types.
You must also run these filters and classifications on a set of controls that share the same ethnic background as your samples.
For each gene, the variant counts of your samples and ethnic controls are then compared using a right-tailed Fisher's exact test.
This generates a p-value for each variant classification.
These p-values are written to a tab-delimited file, which can be imported into a spreadsheet program.
Then you can sort by p-value to find the most significantly mutated genes under the variant category of your choosing.
The most commonly used categories are Total_LoF, Total_Deleterious and Total_Missense.

The annotations used for filtering and mutation classification are better explained at: http://annovar.openbioinformatics.org/en/latest/user-guide/gene/#output-file-1-refseq-gene-annotation

In addition to using the output file that holds the counts of mutations, another file is produced that describes information about each significant variant. This file is intended for further programmatic analysis, but can be viewed in a spreadsheet program, if you wish. This second file is very large and holds redundant data, compared to the VCF file. However, we deemed it necessary to create this file for further analysis.


# HOW DO I USE THESE PROGRAMS?
You must first have an annotated VCF file. The annotations must include ExAC frequency, RefSeq annotation and the deleteriousness score you choose to use (such as metaSVM).
The VCF can have all the possible deleteriousness scores, if you wish.
I have only used VCFs generated by the GATK pipeline and annotated with ANNOVAR.
Your VCF file MUST include the meta-data header generated by ANNOVAR.

In order to run exome_burden_script.py, you must also have a file that delineates low complexity regions on your local file system.
This file must be in python pickle format, as created by the cPickle module.
I have made this file available, named lcr_positions_nov2015.pickle
You must specify, in exome_burden_script.py, the location of this file on your local file system.

You need to run two programs in order: exome_burden_script.py (once on your samples and once on your controls). You then need to run gene_burden_fisher_exact.py, which takes the mutation counts for both your samples and their ethnic control.
You must be running python version 2.7.5 or higher. These programs have NOT been written for Python 3.

Assumptions:
1. The programs in this repository are located on your machine at /home/user/programs
2. Your VCF files (and all other pre-generated data files) are located on your machine at /home/user/data
3. Results of these analysis will be stored at /home/user/results

## Filter and Classify Mutations: exome_burden_script.py
```text
$ /home/user/programs/exome_burden_script.py
usage: exome_burden_script.py parameters [options]

Parameters:
        -f | --frequency        max desired frequency of variants in population databases
        -v | --vcf              VCF file containing variants to be analyzed
        -d | --funcDel          method to use for filtering functional deleteriousness: CADD, metaSVM, radialSVM, MPC
        -p | --pldiff-cutoff    minimum required value of plDiff for a sample to be included in further processing
                 7:low stringency 8:high stringency
Options:
        -o | --outputDir        output directory to write files; default: current directory
        -x | --excel            FLAG: use to cause more header information for display in spreadsheet
        -s | --shift            FLAG: use to specify that nonframeshift indels should be filtered out
        -c | --coverage         required read depth for a variant to be accepted; default: 8
        -a | --africa-special   FLAG: filter specific info fields
```

Here is an example with commonly used options:

```bash
/home/user/programs/exome_burden_script.py \
--frequency 5e-05 \
--vcf /home/user/data/my_annotated_samples.vcf \
--funcDel CADD \
--pldiff-cutoff 8 \
--outputDir /home/user/results \
--shift \
--coverage 10
```

ALL PARAMETERS (but not all OPTIONS) MUST BE SPECIFIED FOR THE PROGRAM TO RUN. Also, do NOT use the "--excel" option when following this work flow. It is intended as a last step in a different work flow not described here.

Regardless of the options you use, this program creates two files in the specified output directory.
1. my_annotated_samples_5e-05_CADD_plDiff8_counts_table.tsv
2. my_annotated_samples_5e-05_CADD_plDiff8_variant.table

The first file contains, for each gene, counts of each variant category described in the table in the previous section of this README file.
The second file is a "melted VCF" in which each variant in each sample has it's own record. This second file was created for further programmatic analysis, not for human consumption.

The general formula for output file names is:
```text
<input_vcf_file_base_name>_<frequency>_<functional_deleteriousness_method>_<plDiff#>_counts_table.tsv
```
This way, you can keep track of which output is from a specific sample and which parameter/option values you used.

You now need to run the program on your controls WITH THE SAME OPTIONS USED BEFORE. Actually, you can use a different output directory, if you wish.

```bash
/home/user/programs/exome_burden_script.py \
--frequency 5e-05 \
--vcf /home/user/data/my_annotated_controls.vcf \
--funcDel CADD \
--pldiff-cutoff 8 \
--outputDir /home/user/results \
--shift \
--coverage 10
```

## Determine p-values: gene_burden_fisher_exact.py
This program requires the two count table files previously generated for your cases and their ethnic controls along with the number of samples in each.

```text
$ /home/user/programs/gene_burden_fisher_exact.py
usage: gene_burden_fisher_exact.py parameters [options]

Parameters:
        -a | --cases-file       CASES gene burden output
        -n | --num-cases        number of cases in gene burden output
        -o | --controls-file    CONTROLS gene burden output
        -m | --num-controls     number of controls in gene burden output
Options:
        -i | --individual-counts         FLAG: independent counts yield independent p-values
```

An example of how to run this program is shown below:
```bash
/home/user/programs/gene_burden_fisher_exact.py \
--cases-file /home/user/results/my_annotated_samples_5e-05_CADD_plDiff8_counts_table.tsv \
--num-cases 100
--controls-file /home/user/results/my_annotated_controls_5e-05_CADD_plDiff8_counts_table.tsv \
--num-controls 2700 \
> /home/user/results/fisher_exact_5e-05_results.tsv
```

This program writes a tab-delimited file that contains, for each gene, the number of counts for cases and controls and their p-value for each variant category described in the table in the first section of this README file.

At this point, you need to decide on a q-value threshold.
You must use a q-value, because we are performing multiple tests against all human genes.
This is true, even if only a subset of these genes appear in the file produced by the gene_burden_fisher_exact.py program.
The threshold we used was 2.5E-6.
This was achieved by dividing 0.05 (a normal p-value threshold) by 20,000 (the approximate number of human genes).

# Utility Programs
## pull_records_by_ethnicity.py
This program was created to pull specific samples from a VCF file to generate a new, smaller VCF file.
We needed to create VCFs containing only samples of a specific ethnicity.
These samples are specified in a file containing one sample name per line.
This program reads in one of these files and a VCF file to generate a new VCF containing only those samples.
This program also removes all records that no longer affect any of the samples included in the VCF file.

For example, say you have a VCF file with five samples, and we only wanted two of them.
This program looks at every record, and rebuilds that record, retaining only the two samples specified.
After this, the program looks at the genotype value of these records.
If none of these records contains an alternate allele, this record is discarded.

```text
$ ./pull_records_by_ethnicity.py
usage: pull_records_by_ethnicity.py parameters

Parameters:
        -v | --vcf              VCF file to read in
        -e | --ethnicity-file   file containing samples to retain
```

Bash command line example:

```bash
cd /home/user/data

/home/user/programs/pull_records_by_ethnicity.py \
--vcf my_annotated_samples.vcf \
--ethnicity-file european_samples.txt \
> /home/user/results/euro_samples.VCF
```

## hashify_lcr_locations.py
This program creates a hash of low complexity regions (LCR) and saves that hash into a pickle file.
This saves time by not having to recreate this data structure every time "exome_burden_script.py" is executed.

```text
$ ./hashify_lcr_locations.py
usage: ./hashify_lcr_locations.py parameters
Parameters:
        -f | --file             file containing tab-separated values
        -o | --output           name of pickle file to write to
```

I am not including a bash example as I feel this program is very straightforward.
You specify a file where each line is a tab-delimited set of chromosome, start and stop positions of a single LCR.
You also specify the name of the pickle file to be generated.
This program is only included to allow you to generate a new pickle file of updated LCRs with the format expected by the "exome_burden_script.py" program.


## gene_mutation_count.py
This program reads in variant table files generated by the program "exome_burden_script.py".
You also specify the corresponding descriptive headers that are used in the output file.
This program writes, to stdout, a tab-delimited file that contains the number of variants for each gene in each variant file.

The variant table file paths are given as a comma-separated list.
Due to bash command line processing, this list cannot contain spaces.
The descriptive headers are also specified in a comma-separated list, with no spaces allowed.

```text
$ /home/user/programs/gene_mutation_count.py
usage: ./gene_mutation_count.py parameters
Parameters:
        -f | --file-list        comma-separated list of variant table files
        -a | --annotation       comma-separated list of names that appear at top of table
                                 names must match the order of files listed for "file-list"
Options:
        -c | --low-complexity-region    include variants that occur in low-complexity regions
```

Bash command line example; Please note the use (and lack of use) of spaces when building the comma-separated lists:

```bash
cd /home/user/results

/home/user/programs/gene_mutation_count.py \
    --file-list my_annotated_samples_5e-05_CADD_plDiff8_counts_table.tsv,\
my_annotated_controls_5e-05_CADD_plDiff8_counts_table.tsv\
    --annotation Cases,Controls
    > gene_mutations_5e-05.tsv
```

The option "low-complexity-region" allows the inclusion of variants that occur in low compleity regions of the genome.
These are not usually counted due to low confidence of variants in these regions.

## count_sample_variants.py
We wished to know how many variants each sample contributed to genes in a given VCF file.
This program counts those variants inside a desired set of genes.
You may specify "all" for the parameter "significant-genes" and variants for all genes will be counted.
Otherwise, you must specify a file containing one gene name per line.
In this case, only the variants in these genes will be counted.

This program reads in the variant table file generated by "exome_burden_script.py".

This program creates a pickle file. Pickle files are merely serialized data, similar to JSON files.
They allow the program to read in a data structure, instead of creating that data structure each time the program is run.
They are not secured files, but rather a faster way to generate a data structure.

This program creates a hash, keyed on sample name, with the value of that key being of the number of variants that sample contributes to your chosen set of genes.
This hash is written to the pickle the user specifies for the "output-file" option.

```text
$ ./count_sample_variants.py
usage: ./count_sample_variants.py parameters
Parameters:
        -v | --variant-file     variant table file generated from "exome_burden_script.py"
        -o | --output-file              full path of file to new pickle file
                                          existing files will be overwritten
        -s | --significant-genes        file containing HUGO names of significant genes to analyze
                                          lines starting with "#" will be ignored
                                          use "all" to count all genes
```

Bash command line example:

```bash
cd /home/user/results

/home/user/programs/count_sample_variants.py \
--variant-file my_annotated_samples_5e-05_CADD_plDiff8_variant.table \
--output-file annotated_samples_signif_gene_variant_count.pickle \
--significant-genes my_signif_genes.txt
```

You will generally create a list of significant genes by looking at the results of the "gene_burden_fisher_exact.py" program.
However, you may put any list of genes in this file you wish.

These pickle files were then used in a Jupyter (formerly IPython) notebook to generate histograms.
These notebooks used the matplotlib module to generate the histograms.
I have included one such notebook as an example of how to use a pickle file generated by this program.

# SPECIAL THANKS
I wish to thank the following people:
* Jungmin Choi, Ph.D. of Dr. Lifton's lab, at the Yale School of Medicine, for help in testing and further improving the program "exome_burden_script.py".
* Francesc Lopez, Ph.D. of the Yale Center for Genome Analysis for the initial explanation of how the "exome_burden_script.py" program should behave and the expected results.
* Dr. Richard Sutton of the Yale School of Medicine, Internal Medicine - Infectious Diseases, for allowing me the opportunity to work on a challenging set of projects in the Sutton Lab.

# THE FINE PRINT
This code is provided "as-is", with no warranty or promise of support.
You assume full responsibility if you choose to run it on any of your systems.
I have had great success in using these, but that is not a guarantee that you will enjoy the same such success.

If you wish to use these programs and encounter problems, I honestly suggest you read through all the documentation, both in this README and in the code files, themselves.
I write code knowing that I will likely come back to it long after I have designed and tested it.
Because of this, I try to write detailed comments that give a good idea of what I was trying to attempt in each program.
This includes not only comments at the top of the program, but also throughout the code.

You may also notice some inconsistency in my use of command line parameters (sometimes I use flags, sometimes I use a catch-all phrase to by-pass a parameter).
I deliberately left these in because code is sometimes created at breakneck speed to answer an immediate question.
So, you can see how sloppy I get, when designing under such conditions.
